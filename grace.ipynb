{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trying to implement GRACE using torchjpeg\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "import torchjpeg.codec\n",
    "from gradients.grace import *\n",
    "from lvc.lvc import *\n",
    "from models.model import plot_channels\n",
    "from transforms.dct import *\n",
    "from transforms.metrics import mse_psnr\n",
    "from utils import block_process, Size\n",
    "\n",
    "INP_FILE = \"/home/jakub/data/kodim/raw/kodim23.png\"\n",
    "ENC_FILE = \"kodim_out.jpg\"\n",
    "OUT_FILE = \"kodim_out_dec.png\"\n",
    "# RUN_DIR = Path(\"/media/data1/jakub/lvc-for-cv/experiments/runs/run43_keep\")  # full-frame DCT\n",
    "# RUN_DIR = Path(\"/media/data1/jakub/lvc-for-cv/experiments/runs/run46_keep\")  # full-frame DCT with 255 range\n",
    "# RUN_DIR = Path(\"/media/data1/jakub/lvc-for-cv/experiments/runs/run44_keep\")  # block-based DCT\n",
    "# RUN_DIR = Path(\"/media/data1/jakub/lvc-for-cv/experiments/runs/run45_keep\") # subtracted mean => less q dynamic range\n",
    "# RUN_DIR = Path(\"/media/data1/jakub/lvc-for-cv/experiments/runs/run47_keep\")  # all probe variants\n",
    "# RUN_DIR = Path(\n",
    "#     \"/media/data1/jakub/lvc-for-cv/experiments/runs/run48_keep\"\n",
    "# )  # all probe variants\n",
    "# RUN_DIR = Path(\"/media/data1/jakub/lvc-for-cv/experiments/runs/run254\")  # all probe variants, probed with 1 image\n",
    "# RUN_DIR = Path(\"/media/data1/jakub/lvc-for-cv/experiments/runs/run256\")  # all probe variants, probed with 1 image, DCT 16x16\n",
    "RUN_DIR = Path(\n",
    "    \"/home/jakub/git/nn-spectral-sensitivity/experiments_tupu/runs/run10_keep\"\n",
    ")  # all probe variants\n",
    "\n",
    "QUALITY = 100\n",
    "COLOR_SAMP_FACTOR_VERTICAL = 1\n",
    "COLOR_SAMP_FACTOR_HORIZONTAL = 1\n",
    "\n",
    "W_HUMAN = (0.299, 0.587, 0.114)\n",
    "\n",
    "# Get probe results\n",
    "model = \"fastseg_small\"\n",
    "mode = 444\n",
    "dist = \"dist_abs\"\n",
    "sub = \"submean\"\n",
    "dct_size = Size(h=8, w=8)\n",
    "dctsz = \"ff\"  # bb8x8 or ff\n",
    "sc = 255  # 1 or 255\n",
    "\n",
    "# generated by experiments.probe_only():\n",
    "try:\n",
    "    probe_results = torch.load(RUN_DIR / f\"probe_result_full_{model}_{mode}.pt\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        probe_results = torch.load(\n",
    "            RUN_DIR / f\"probe_result_full_{model}_{mode}_{dist}_{sub}.pt\"\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        probe_results = torch.load(\n",
    "            RUN_DIR / f\"probe_result_full_{model}_{mode}_{dist}_{sub}_{dctsz}_sc{sc}.pt\"\n",
    "        )\n",
    "\n",
    "W = probe_results[\"W\"]\n",
    "print(f\"W: {W}\")\n",
    "\n",
    "norm_abs = lambda tensor: tensor.abs().mean()\n",
    "# norm_sq = lambda tensor: tensor.square().sum().sqrt()\n",
    "\n",
    "g = probe_results[\"grads_yuv\"]\n",
    "dct = probe_results[\"dct_yuv\"]\n",
    "print(f\"g: {g.shape}, min: {g.min()}, max: {g.max()}\")\n",
    "print(f\"dct: {dct.shape}, min: {dct.min()}, max: {dct.max()}\")\n",
    "\n",
    "if dctsz == \"ff\":\n",
    "    # average DCT/grad obtained with full-frame DCT\n",
    "    g_block_norm = block_process(\n",
    "        g, (int(g.shape[1] / dct_size.h), int(g.shape[2] / dct_size.w)), norm_abs\n",
    "    )\n",
    "    dct_block_norm = block_process(\n",
    "        dct,\n",
    "        (int(dct.shape[1] / dct_size.h), int(dct.shape[2] / dct_size.w)),\n",
    "        norm_abs,\n",
    "    )\n",
    "else:\n",
    "    # use this for DCT/grad obtained with block DCT\n",
    "    sz = math.ceil(g.shape[0] / 3)\n",
    "    g_block_norm = torch.stack([x.abs().mean(dim=0) for x in g.split(sz)])\n",
    "    dct_block_norm = torch.stack([x.abs().mean(dim=0) for x in dct.split(sz)])\n",
    "\n",
    "mx = torch.tensor([g_block_norm.max(), dct_block_norm.max()]).max()\n",
    "mn = torch.tensor([g_block_norm.min(), dct_block_norm.min()]).min()\n",
    "\n",
    "# Bring to 16 bits\n",
    "# g_block_norm = (2**16 - 1) * (g_block_norm - mn) / (mx - mn) + 1\n",
    "# dct_block_norm = (2**16 - 1) * (dct_block_norm - mn) / (mx - mn) + 1\n",
    "\n",
    "print(f\"g_block_norm: {g_block_norm.shape}\")\n",
    "print(f\"dct_block_norm: {dct_block_norm.shape}\")\n",
    "print(f\"total dynamic range: {mn} -- {mx}: {mx / mn}\")\n",
    "assert g_block_norm.shape[1] == dct_size.h\n",
    "assert g_block_norm.shape[2] == dct_size.w\n",
    "assert dct_block_norm.shape[1] == dct_size.h\n",
    "assert dct_block_norm.shape[2] == dct_size.w\n",
    "plot_channels(\n",
    "    g_block_norm.cpu().detach().numpy(), \"g_block_norm\", width=500, height=1000\n",
    ")\n",
    "plot_channels(\n",
    "    dct_block_norm.cpu().detach().numpy(),\n",
    "    \"dct_block_norm\",\n",
    "    log=True,\n",
    "    width=500,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "approx_quant = False\n",
    "\n",
    "if approx_quant:\n",
    "    # Choose B such that with approximate quantization table max. q is 255. This\n",
    "    # gives a reasonable initial ballpark value.\n",
    "    B = 255 * g_block_norm.min().item() * dct_size.w * dct_size.h / 2\n",
    "else:\n",
    "    B = 5e-5\n",
    "\n",
    "print(f\"B: {B}\")\n",
    "\n",
    "### TEST\n",
    "q2 = torch.zeros_like(g)\n",
    "d2 = torch.zeros_like(g)\n",
    "bounded2 = torch.zeros_like(g)\n",
    "max_loss_increase2 = torch.zeros_like(g)\n",
    "\n",
    "for i, (g_norm, dct_norm) in enumerate(zip(g_block_norm, dct_block_norm)):\n",
    "    print(i)\n",
    "    q2[i], d2[i], bounded2[i], max_loss_increase2[i] = get_quant_table(g, dct, B, do_print=False)\n",
    "\n",
    "q2_block_norm = block_process(\n",
    "    q2, (int(q2.shape[1] / dct_size.h), int(q2.shape[2] / dct_size.w)), norm_abs\n",
    ")\n",
    "\n",
    "plot_channels(\n",
    "    q2_block_norm.cpu().detach().numpy(),\n",
    "    f\"Q2 table (Y/U/V), B = {B}\",\n",
    "    log=False,\n",
    "    width=500,\n",
    "    height=1000,\n",
    "    # save=\"experiments/plots/grace_q_table.png\",\n",
    "    save=None,\n",
    ")\n",
    "\n",
    "### TEST\n",
    "\n",
    "\n",
    "q = torch.zeros_like(g_block_norm)\n",
    "d = torch.zeros_like(g_block_norm)\n",
    "bounded = torch.zeros_like(g_block_norm)\n",
    "max_loss_increase = torch.zeros_like(g_block_norm)\n",
    "\n",
    "for i, (g_norm, dct_norm) in enumerate(zip(g_block_norm, dct_block_norm)):\n",
    "    if approx_quant:\n",
    "        q[i], d[i] = get_quant_table_approx(g_norm, B)\n",
    "    else:\n",
    "        q[i], d[i], bounded[i], max_loss_increase[i] = get_quant_table(\n",
    "            g_norm, dct_norm, B, do_print=True\n",
    "        )\n",
    "\n",
    "QUANT = q.type(torch.int16)\n",
    "\n",
    "plot_channels(\n",
    "    q.cpu().detach().numpy(),\n",
    "    f\"Q table (Y/U/V), B = {B}\",\n",
    "    log=False,\n",
    "    width=500,\n",
    "    height=1000,\n",
    "    save=\"experiments/plots/grace_q_table.png\",\n",
    ")\n",
    "print(\"dct_block_norm:\")\n",
    "print(dct_block_norm)\n",
    "print(\"g_block_norm:\")\n",
    "print(g_block_norm)\n",
    "print(\"d:\")\n",
    "print(d)\n",
    "print(\"q:\")\n",
    "print(q)\n",
    "print(f\"q dynamic range: {q.max() / q.min()}\")\n",
    "\n",
    "qdct_block_norm = (dct_block_norm / q).round()\n",
    "print(\"quantized DCT block norm\")\n",
    "print(qdct_block_norm)\n",
    "# plot_channels(qdct_block_norm, \"quantized DCT block norm\")\n",
    "\n",
    "print(\"bounded:\")\n",
    "print(bounded)\n",
    "print(\"q - 2 * dct_block_norm (should be negative or 0)\")\n",
    "print(q - 2 * dct_block_norm)\n",
    "print(\"d <= max_loss_increase\")\n",
    "print(d <= (g_block_norm * dct_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run JPEG encode + decode (requires first cell)\n",
    "\n",
    "from transforms import Interpolate\n",
    "\n",
    "# Read image\n",
    "# INP_FILE = \"probed_cityscapes_images_batchsize1/batch0_img0.png\"\n",
    "img_rgb = to_tensor(Image.open(INP_FILE))\n",
    "\n",
    "if img_rgb.shape[0] > 3:\n",
    "    img_rgb = img_rgb[:3]\n",
    "\n",
    "# RGB -> YUV\n",
    "rgb2yuv = RgbToYcbcr(W)\n",
    "img_yuv = rgb2yuv(img_rgb)\n",
    "print(f\"img_yuv {img_yuv.shape}\")\n",
    "\n",
    "# encode\n",
    "dimensions, quantization, Y_coefficients, CbCr_coefficients, enc_data = (\n",
    "    torchjpeg.codec.quantize_at_quality_custom(\n",
    "        img_yuv,\n",
    "        QUALITY,\n",
    "        QUANT,\n",
    "        COLOR_SAMP_FACTOR_VERTICAL,\n",
    "        COLOR_SAMP_FACTOR_HORIZONTAL,\n",
    "    )\n",
    ")\n",
    "\n",
    "# compare JPEG DCT against our DCT\n",
    "y_dct = Y_coefficients.reshape(-1, 8, 8)\n",
    "cb_dct = CbCr_coefficients[0].reshape(-1, 8, 8)\n",
    "cr_dct = CbCr_coefficients[1].reshape(-1, 8, 8)\n",
    "\n",
    "\n",
    "dct_ycbcr = torch.stack(\n",
    "    [\n",
    "        y_dct.float().abs().mean(dim=0).squeeze(),\n",
    "        cb_dct.float().abs().mean(dim=0).squeeze(),\n",
    "        cr_dct.float().abs().mean(dim=0).squeeze(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mask = torch.ones_like(dct_ycbcr).bool()\n",
    "mask[:, 0, 0] = False  # ignore DC coeff.\n",
    "minval = 1\n",
    "maxval = 100\n",
    "\n",
    "# dct_ycbcr = Interpolate(minval, maxval, mask)(dct_ycbcr)\n",
    "# dct_ycbcr[:, 0, 0] = minval\n",
    "plot_channels(\n",
    "    dct_ycbcr.cpu().numpy(),\n",
    "    \"DCT after JPEG\",\n",
    "    log=False,\n",
    "    width=500,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "dct_grace = dct_block_norm\n",
    "# dct_grace = Interpolate(minval, maxval, mask)(dct_grace)\n",
    "# dct_grace[:, 0, 0] = minval\n",
    "plot_channels(\n",
    "    dct_grace.cpu().detach().numpy(),\n",
    "    \"DCT from probe\",\n",
    "    log=False,\n",
    "    width=500,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "diff = dct_ycbcr - dct_grace\n",
    "plot_channels(diff.cpu().numpy(), \"Diff\", width=500, height=1000)\n",
    "\n",
    "print(\"Encoded JPEG size:\", enc_data.shape, enc_data.dtype)\n",
    "\n",
    "print(\"quantization:\")\n",
    "print(quantization)\n",
    "\n",
    "torchjpeg.codec.write_coefficients_custom(\n",
    "    ENC_FILE, dimensions, quantization, Y_coefficients, CbCr_coefficients\n",
    ")\n",
    "\n",
    "# decode\n",
    "dimensions, quantization, Y_coefficients, CbCr_coefficients = (\n",
    "    torchjpeg.codec.read_coefficients(ENC_FILE)\n",
    ")\n",
    "print(\"Y\", Y_coefficients.shape, \"CbCr\", CbCr_coefficients.shape)\n",
    "out_yuv = torchjpeg.codec.reconstruct_full_image(\n",
    "    Y_coefficients, quantization, CbCr_coefficients, dimensions, raw=True\n",
    ")\n",
    "print(\"out_yuv:\", out_yuv.shape)\n",
    "\n",
    "# YUV -> RGB\n",
    "yuv2rgb = YcbcrToRgb(W)\n",
    "out_rgb = yuv2rgb(out_yuv).clamp(0.0, 1.0)\n",
    "\n",
    "to_pil_image(out_rgb).save(OUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1a3ddf04b14fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing block DCT (requires first cell)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chunk_size = 64\n",
    "mode = 444\n",
    "dct_size = (8, 8)\n",
    "device = \"cpu\"\n",
    "\n",
    "preprocess = nn.ModuleList(\n",
    "    [\n",
    "        WrapMetadata(chunk_size),\n",
    "        Pad(mode, dct_size),\n",
    "        RgbToYcbcrMetadata(W),\n",
    "        ChunkSplit(dct_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "postprocess = nn.ModuleList(\n",
    "    [\n",
    "        ChunkCombine(mode, device, dct_size, is_half=False),\n",
    "        YcbcrToRgbMetadata(W),\n",
    "        Crop(),\n",
    "        StripMetadata(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dct = Dct(norm=\"ortho\")\n",
    "idct = Idct(norm=\"ortho\")\n",
    "\n",
    "print(\"yuv shape:\", img_yuv.shape)\n",
    "inp = img_yuv\n",
    "for layer in preprocess:\n",
    "    if type(inp) == tuple:\n",
    "        inp = layer(*inp)\n",
    "    else:\n",
    "        inp = layer(inp)\n",
    "\n",
    "inp, meta = inp\n",
    "\n",
    "print(\"before DCT:\", inp.shape)\n",
    "dct_block = dct(inp)\n",
    "print(\"block DCT:\", dct_block.shape)\n",
    "\n",
    "out_dct = (dct_block, meta)\n",
    "for layer in postprocess:\n",
    "    if type(out_dct) == tuple:\n",
    "        out_dct = layer(*out_dct)\n",
    "    else:\n",
    "        out_dct = layer(out_dct)\n",
    "\n",
    "idct_block = idct(dct_block)\n",
    "print(\"block IDCT:\", idct_block.shape)\n",
    "\n",
    "out = (idct_block, meta)\n",
    "for layer in postprocess:\n",
    "    if type(out) == tuple:\n",
    "        out = layer(*out)\n",
    "    else:\n",
    "        out = layer(out)\n",
    "\n",
    "print(\"out:\", out.shape)\n",
    "mse, psnr = mse_psnr(out, img_yuv)\n",
    "print(f\"psnr: {psnr:.3f} dB\")\n",
    "\n",
    "fig = px.imshow(out_dct.permute(1, 2, 0).cpu().detach().numpy(), title=\"dct\")\n",
    "fig.show()\n",
    "fig = px.imshow(out.permute(1, 2, 0).cpu().detach().numpy(), title=\"out\")\n",
    "fig.show()\n",
    "\n",
    "# print(\"trying block DCT\")\n",
    "# yuv_block2 = dct_2d_block(img_yuv.unsqueeze(0), \"ortho\", 8, 8)\n",
    "# print(\"yuv block:\", yuv_block2.shape)\n",
    "# idct_block = idct_2d(yuv_block2)\n",
    "# print(\"idct block:\", idct_block.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbad8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for JPEG compression ratio\n",
    "# Requires first cell to get initial B estimate and W\n",
    "\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.q_search import find_val, ParamConfig\n",
    "\n",
    "# cpus = [0]\n",
    "# os.sched_setaffinity(0, set(cpus))\n",
    "# affinity = os.sched_getaffinity(0)\n",
    "# print(f\"Running on CPUs: {affinity}\")\n",
    "# torch.set_num_threads(len(cpus))\n",
    "\n",
    "nbits_per_sym = [2, 4, 6]  # 4-, 16- and 64-QAM\n",
    "# target_crs = [0.03125, 0.06250, 0.12500, 0.25000, 0.50000, 1.00000]\n",
    "target_crs = [0.50000, 1.00000]\n",
    "nimages = 1\n",
    "res = {}\n",
    "\n",
    "params_jpeg: ParamConfig = {\n",
    "    \"codec\": \"jpeg\",\n",
    "    \"init\": 50,\n",
    "    \"min\": 1,\n",
    "    \"max\": 100,\n",
    "    \"g_block_norm\": None,\n",
    "    \"W\": None,\n",
    "    \"valname\": \"Q\",\n",
    "    \"fmt\": \"{:3}\",\n",
    "}\n",
    "\n",
    "params_grace: ParamConfig = {\n",
    "    \"codec\": \"grace\",\n",
    "    \"init\": B,\n",
    "    \"min\": B / 255,\n",
    "    \"max\": B * 255,\n",
    "    \"g_block_norm\": g_block_norm,\n",
    "    \"W\": W,\n",
    "    \"valname\": \"B\",\n",
    "    \"fmt\": \"{:12.5e}\",\n",
    "}\n",
    "\n",
    "params = params_grace\n",
    "\n",
    "for nbits, target_cr in itertools.product(nbits_per_sym, target_crs):\n",
    "    q, cr, psnr = find_val(\n",
    "        target_cr,\n",
    "        nbits,\n",
    "        params,\n",
    "        \"coco\",\n",
    "        nimages=nimages,\n",
    "        do_print=True,\n",
    "    )\n",
    "    res[(nbits, target_cr)] = (q, cr)\n",
    "    qfmt = params[\"fmt\"].format(q)\n",
    "    print(\n",
    "        f\"{2**nbits:2}-QAM, target LCT CR: {target_cr:.5f}, actual LCT CR: {cr:.5f}, Q: {qfmt}, {psnr:7.3f} dB PSNR\"\n",
    "    )\n",
    "\n",
    "print(\"Done\")\n",
    "torch.save(res, \"q_search_jpeg_grace.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import q_search\n",
    "\n",
    "outdir = Path(\"experiments/q_search_test\")\n",
    "outdir.mkdir(exist_ok=True, parents=True)\n",
    "q_search(outdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
