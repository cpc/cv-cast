{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distribution of the difference between input and reconstructed images\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2 \n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from scipy.optimize import curve_fit\n",
    "from torchvision import transforms\n",
    "\n",
    "from lvc import create_lvc\n",
    "from utils import set_device\n",
    "\n",
    "device = set_device()\n",
    "\n",
    "# img_dir = Path(\"/home/jakub/pictures/kodim/raw\")\n",
    "img_dir = Path(\"/home/kubouch/pictures/kodim/raw\")\n",
    "image_names = list(img_dir.glob(\"*.png\"))\n",
    "images = torch.stack([\n",
    "    # transforms.CenterCrop((288, 352))(\n",
    "    transforms.Resize((512, 768))(\n",
    "    # transforms.Resize((288, 672))(\n",
    "        transforms.ToTensor()(\n",
    "            Image.open(image_name).convert(mode=\"RGB\")\n",
    "        )\n",
    "    ) \n",
    "    for image_name in image_names \n",
    "])\n",
    "print(\"Images: \", images.shape)\n",
    "\n",
    "lvc_params_zf = {\n",
    "    \"csnr_db\": 0,\n",
    "    \"cr\": 1.0,\n",
    "    \"mode\": 444,\n",
    "    # \"chunk_w\": 32,\n",
    "    # \"chunk_h\": 32,\n",
    "    \"nchunks\": 64,\n",
    "    \"seed\": 42,\n",
    "    \"estimator\": \"zf\",  # llse\n",
    "    \"packet_loss\": None, #0.0, \n",
    "    # \"dct_w\": 16,\n",
    "    # \"dct_h\": 16,\n",
    "    # \"grouping\": \"vertical_uv\",\n",
    "}\n",
    "\n",
    "results_zf = {\"noise\": []}\n",
    "results_llse = {\"noise\": []}\n",
    "\n",
    "lvc_params_llse = lvc_params_zf\n",
    "lvc_params_llse[\"estimator\"] = \"llse\"\n",
    "\n",
    "lvc_zf = create_lvc(lvc_params_zf, device, half=False, results=results_zf)\n",
    "lvc_llse = create_lvc(lvc_params_llse, device, half=False, results=results_llse)\n",
    "\n",
    "res_zf = lvc_zf(images)\n",
    "res_llse = lvc_llse(images)\n",
    "\n",
    "noise = torch.stack(results_zf[\"noise\"])\n",
    "print(\"noise mean: {}\".format(noise.mean()))\n",
    "\n",
    "TODO: Do this per chunk, not on whole image\n",
    "diff_zf = res_zf - images\n",
    "diff_llse = res_llse - images\n",
    "\n",
    "nbins = 128\n",
    "    \n",
    "def plot_gaussian_histogram(ax, values: torch.Tensor, nbins: int):\n",
    "    def gaussian(x, mean, amplitude, stddev):\n",
    "        return amplitude \\\n",
    "            / (stddev * np.sqrt(2 * np.pi)) \\\n",
    "            * np.exp(-0.5 * ((x - mean) / stddev)**2) \n",
    "\n",
    "    bin_heights, bin_borders, _ = ax.hist(values.reshape(-1).numpy(), bins=nbins)\n",
    "    bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "    popt_gaussian, _ = curve_fit(\n",
    "        gaussian, \n",
    "        bin_centers, \n",
    "        bin_heights, \n",
    "        p0=[0.0, 0.0, 1.0],\n",
    "    )\n",
    "\n",
    "    x = np.linspace(bin_borders[0], bin_borders[-1], 10000)\n",
    "    ax.plot(\n",
    "        x, \n",
    "        gaussian(x, *popt_gaussian), \n",
    "        \"r-\", \n",
    "        label=\"mean={:5.3f}, stddev={:5.3f}\".format(\n",
    "            tuple(popt_gaussian)[0], \n",
    "            tuple(popt_gaussian)[2],\n",
    "        )\n",
    "    )\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle(\"Difference distributions\")\n",
    "\n",
    "axs[0].set_title(\"Difference Histogram (ZF)\")\n",
    "plot_gaussian_histogram(axs[0], diff_zf, nbins)\n",
    "\n",
    "axs[1].set_title(\"Difference Histogram (LLSE)\")\n",
    "plot_gaussian_histogram(axs[1], diff_llse, nbins)\n",
    "\n",
    "axs[2].set_title(\"Noise histogram\")\n",
    "plot_gaussian_histogram(axs[2], noise, nbins)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating the power savings\n",
    "#\n",
    "# We're minimizing the power given a constraint on the total distortion C\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2 \n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.nn import NLLLoss, LogSoftmax\n",
    "from torchvision import transforms\n",
    "\n",
    "from lvc import Pad, LvcEncode, RgbToYcbcrMetadata, Downsample, SubtractMean, \\\n",
    "    ChunkSelect, ChunkSplit, DCT, DctBlock, Metadata, YuvImage\n",
    "from probe import probe_images\n",
    "from test_nn import WEIGHTS, Dct, Idct, get_imagenet_labels\n",
    "from utils import set_device\n",
    "\n",
    "device = set_device()\n",
    "\n",
    "w_human = torch.Tensor([0.299, 0.587, 0.114])\n",
    "norm = \"ortho\"\n",
    "power = 1.0\n",
    "do_print = False\n",
    "half = False\n",
    "references = None\n",
    "results = None\n",
    "\n",
    "csnr_db = 10\n",
    "cr = 0.5\n",
    "yuv_mode = 444\n",
    "# chunk_w = 32\n",
    "# chunk_h = 32\n",
    "nchunks = 64\n",
    "seed = 42\n",
    "estimator = \"zf\"  # llse\n",
    "packet_loss = None #0.0 \n",
    "# dct_w = 16\n",
    "# dct_h = 16\n",
    "# grouping = vertical_uv\n",
    "chunk_size = nchunks\n",
    "dct_size = None\n",
    "\n",
    "if dct_size is None:\n",
    "    dct_layers = (DCT(half, norm), ChunkSplit(dct_size, do_print=do_print))\n",
    "else:\n",
    "    grouping = \"vertical_uv\"\n",
    "    dct_layers = (\n",
    "        ChunkSplit(dct_size, do_print=do_print),\n",
    "        DctBlock(\n",
    "            dct_size, yuv_mode, grouping, is_half=half, norm=norm, do_print=do_print\n",
    "        ),\n",
    "    )\n",
    "\n",
    "lvc_encoder = LvcEncode(\n",
    "    [\n",
    "        Pad(yuv_mode, dct_size, do_print=do_print),\n",
    "        RgbToYcbcrMetadata(w=w_human),\n",
    "        Downsample(mode=yuv_mode),\n",
    "        SubtractMean(device),\n",
    "        dct_layers[0],\n",
    "        dct_layers[1],\n",
    "        ChunkSelect(cr, do_print=do_print),\n",
    "    ],\n",
    "    references,\n",
    "    None, #results,\n",
    "    chunk_size,\n",
    ").to(device, non_blocking=True)\n",
    "\n",
    "img_size = (3, 256, 256)  # size of resized images\n",
    "inp_size = (3, 224, 224)  # size of cropped images fed into NN\n",
    "nimages = 10\n",
    "# img_dir = Path(\"/home/jakub/pictures/kodim/raw\")\n",
    "# image_names = list(img_dir.glob(\"*23.png\"))\n",
    "# img_dir = Path(\"/mnt/1tb_storage/data/imagenet_subsets/subset_10class_1000perclass/val\")\n",
    "img_dir = Path(\"/home/kubouch/data/imagenet_subsets/subset_10class_1000perclass/val\")\n",
    "image_names = list(img_dir.glob(\"*/*.jpg\"))[:nimages]\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size[1]),\n",
    "        transforms.CenterCrop(inp_size[1]),\n",
    "        transforms.ToTensor(),\n",
    "        # RgbToYcbcr(W),\n",
    "        #  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "images = torch.stack([\n",
    "    preprocess(Image.open(image_name).convert(mode=\"RGB\")).to(device)\n",
    "    for image_name in image_names \n",
    "])\n",
    "print(\"Images: \", images.shape)\n",
    "\n",
    "results = lvc_encoder(images)\n",
    "metadatas = [m for _, m in results]\n",
    "\n",
    "show_gradients = False\n",
    "\n",
    "# dct_enc, dct_dec = create_dct(\n",
    "#     (dct_size, dct_size),\n",
    "#     dct_size * dct_size,\n",
    "#     w_human,\n",
    "#     device,\n",
    "#     False,\n",
    "#     do_print=False,\n",
    "# )\n",
    "\n",
    "model_name = \"alexnet\"\n",
    "model = torch.hub.load(\n",
    "    \"pytorch/vision:v0.13.1\", model_name, weights=WEIGHTS[model_name]\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "activ = LogSoftmax(dim=1)\n",
    "loss_func = NLLLoss()\n",
    "\n",
    "labels = get_imagenet_labels(image_names)  # np.zeros(len(pics))\n",
    "\n",
    "grads = probe_images(\n",
    "    model,\n",
    "    image_names,\n",
    "    labels,\n",
    "    preprocess,\n",
    "    Dct(),\n",
    "    Idct(),\n",
    "    activ,\n",
    "    loss_func,    \n",
    "    device=device,\n",
    ")\n",
    "print(\"Probed gradients:\", grads.shape)\n",
    "\n",
    "report_n = 0\n",
    "print(\"Image: \", '/'.join(Path(image_names[report_n]).parts[-2:]))\n",
    "metadata = metadatas[report_n]\n",
    "var_all = metadata.all_variances\n",
    "g = grads[report_n]\n",
    "\n",
    "g_split = ChunkSplit(dct_size)\n",
    "g_metadata = Metadata(metadata.image_size, metadata.chunk_size)\n",
    "\n",
    "g_chunks, _ = g_split(YuvImage(g[0], g[1], g[2]), g_metadata)\n",
    "g_chunks_mean = g_chunks.abs().mean(dim=(1,2))\n",
    "\n",
    "var_t = var_all[metadata.bitmap]\n",
    "var_d = var_all[~metadata.bitmap]\n",
    "g_t = g_chunks_mean[metadata.bitmap[:]]\n",
    "g_d = g_chunks_mean[~metadata.bitmap[:]]\n",
    "# K = len(var_t)\n",
    "sigma2 = power / math.pow(10, csnr_db / 10) # noise power\n",
    "C = torch.logspace(start=-3.8, end=-2, steps=100).to(device)\n",
    "\n",
    "# Theorem 3\n",
    "P_zf = sigma2 \\\n",
    "    * (g_t.square() * var_t).pow(1/3).sum().pow(3) \\\n",
    "    / (C - (g_d * var_d.sqrt()).sum()).square()\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(7, 7))\n",
    "axs.plot(C.cpu(), P_zf.cpu())\n",
    "axs.set_xscale(\"log\")\n",
    "axs.grid(which=\"major\")\n",
    "axs.grid(which=\"minor\")\n",
    "axs.set_xlabel(\"C\")\n",
    "axs.set_ylabel(\"P\")\n",
    "plt.show()\n",
    "\n",
    "# Theorem 1\n",
    "import numpy as np\n",
    "\n",
    "def pert(g, var_all, csnr_db, k, P=1.0):\n",
    "    _, top_chunks_indices = var_all.topk(k)\n",
    "    top_chunks_indices, _ = top_chunks_indices.sort()\n",
    "    top_chunks = var_all[top_chunks_indices]\n",
    "\n",
    "    bitmap = torch.zeros_like(var_all, dtype=torch.bool)\n",
    "    bitmap[top_chunks_indices] = True\n",
    "\n",
    "    var_t = var_all[bitmap]\n",
    "    var_d = var_all[~bitmap]\n",
    "    g_t = g[bitmap[:]]\n",
    "    g_d = g[~bitmap[:]]\n",
    "    \n",
    "    tmp_sum = (g_t.square() * var_t).pow(2 / 3).sum()\n",
    "\n",
    "    beta = torch.sqrt(P / tmp_sum * (g_t / var_t).pow(2 / 3))\n",
    "    \n",
    "    sigma2 = P / math.pow(10, csnr_db / 10) # noise power\n",
    "    pert_t = (g_t * math.sqrt(sigma2) / beta).sum()\n",
    "    pert_d = (g_d * var_d).sum()\n",
    "    \n",
    "    return pert_t + pert_d\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ks = np.arange(1, len(var_all) + 1)\n",
    "for csnr_db in [0, 5, 10, 20, 30]:\n",
    "    perts = [pert(g_chunks_mean, var_all, csnr_db, k).cpu() for k in ks]\n",
    "    min_k = ks[np.argmin(perts)]\n",
    "    print(f\"CSNR {csnr_db:2d} dB: min. pert. at K = {min_k}\")\n",
    "    axs.plot(ks, perts, label=f\"{csnr_db} dB\")\n",
    "    \n",
    "axs.legend()\n",
    "axs.grid(which=\"major\")\n",
    "axs.grid(which=\"minor\")\n",
    "axs.set_xlabel(\"K\")\n",
    "axs.set_ylabel(\"perturbation\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying optimal power allocation\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2 \n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "from torchsummary import summary\n",
    "\n",
    "from lvc import Pad, LvcEncode, RgbToYcbcrMetadata, Downsample, SubtractMean, \\\n",
    "    ChunkSelect, ChunkSplit, DCT, DctBlock, Metadata, YuvImage\n",
    "from test_nn import WEIGHTS, Dct, Idct, get_imagenet_labels\n",
    "from utils import set_device\n",
    "\n",
    "device = set_device()\n",
    "\n",
    "img_size = (3, 256, 256)  # size of resized images\n",
    "inp_size = (3, 224, 224)  # size of cropped images fed into NN\n",
    "nimages = 10\n",
    "\n",
    "# img_dir = Path(\"/home/jakub/pictures/kodim/raw\")\n",
    "# image_names = list(img_dir.glob(\"*23.png\"))\n",
    "# img_dir = Path(\"/mnt/1tb_storage/data/imagenet_subsets/subset_10class_1000perclass/val\")\n",
    "img_dir = Path(\"/home/kubouch/data/imagenet_subsets/subset_10class_1000perclass/val\")\n",
    "image_names = list(img_dir.glob(\"*/*.jpg\"))[:nimages]\n",
    "\n",
    "weights = models.ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1\n",
    "model = models.shufflenet_v2_x1_0(weights)\n",
    "# model = model.to(device)\n",
    "model.eval()\n",
    "preprocess = weights.transforms()\n",
    "from torchvision.io import read_image\n",
    "img = read_image(str(image_names[5]))\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")\n",
    "plt.figure()\n",
    "plt.imshow(transforms.ToPILImage()(img))\n",
    "# transforms.Compose(\n",
    "#     [\n",
    "#         transforms.Resize(img_size[1]),\n",
    "#         transforms.CenterCrop(inp_size[1]),\n",
    "#         transforms.ToTensor(),\n",
    "#         # RgbToYcbcr(W),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# batch_size = min(len(image_names), 32)\n",
    "# labels = get_imagenet_labels(image_names)  # np.zeros(len(pics))\n",
    "# dataset = LocalDataset(image_names, labels=labels, transform=preprocess)\n",
    "# dataset = datasets.ImageNet(\n",
    "#     \"/home/kubouch/data/imagenet_subsets/subset_10class_1000perclass\",\n",
    "#     split=\"val\",\n",
    "#     transform=preprocess,\n",
    "# )\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=True)\n",
    "# summary(model, inp_size)\n",
    "# activ = LogSoftmax(dim=1)\n",
    "# activ = lambda x: x\n",
    "# loss_func = NLLLoss()\n",
    "# loss_func = CrossEntropyLoss()\n",
    "\n",
    "# loss, acc, class_correct, class_total = val(\n",
    "#     model, device, dataloader, activ, loss_func, 10)\n",
    "# print(f\"Loss: {loss}, Acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (FastSeg testing old vs new results)\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2 \n",
    "%reset -f\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from models import fastseg, get_model\n",
    "from models.model import plot_channels \n",
    "from utils import set_device\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "color_space = \"yuv\"\n",
    "num_batches = 8\n",
    "batch_size = 8\n",
    "num_batches_probe = 1\n",
    "batch_size_probe = 1\n",
    "\n",
    "device = set_device(\"cuda\", do_print=True)\n",
    "do_print = False\n",
    "show_plots = False\n",
    "\n",
    "for variant in [\"small\", \"large\"]:\n",
    "    result_orig = fastseg.main(\n",
    "        mode=\"eval\", \n",
    "        device=device, \n",
    "        num_batches=num_batches, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        color_space=color_space,\n",
    "        do_print=do_print,\n",
    "        variant=variant,\n",
    "    )\n",
    "\n",
    "    result_probe = fastseg.main(\n",
    "        mode=\"probe\",\n",
    "        device=device,\n",
    "        num_batches=num_batches_probe, \n",
    "        batch_size=batch_size_probe,\n",
    "        num_workers=0,\n",
    "        color_space=color_space,\n",
    "        do_print=do_print,\n",
    "        show_plots=show_plots,\n",
    "        variant=variant,\n",
    "    )\n",
    "\n",
    "    plot_channels(\n",
    "        result_probe[\"grads_norm\"].cpu().numpy(),\n",
    "        [\n",
    "            \"block_norm DCT gradient (Y)\",\n",
    "            \"block_norm DCT gradient (U)\",\n",
    "            \"block_norm DCT gradient (V)\",\n",
    "        ],\n",
    "        0,\n",
    "        show=True,\n",
    "        save=f\"fastseg{variant}_grads_norm_test_old.png\",\n",
    "        tight_layout=False,\n",
    "    )\n",
    "\n",
    "    if variant == \"large\":\n",
    "        snapshot = Path.home() / \"data/models/fastseg/raw/large/best_checkpoint_ep172.pth\"\n",
    "    else:\n",
    "        snapshot = Path.home() / \"data/models/fastseg/raw/small/best_checkpoint_ep171.pth\"\n",
    "\n",
    "    config = {\n",
    "        \"name\": \"fastseg\",\n",
    "        \"variant\": variant,\n",
    "        \"snapshot\": snapshot,\n",
    "    }\n",
    "\n",
    "    model = get_model(config, device, num_batches, batch_size, color_space=color_space)\n",
    "    result_new_orig = model.eval()\n",
    "\n",
    "    model_probe = get_model(config, device, num_batches_probe, batch_size_probe, color_space=color_space)\n",
    "    result_new_probe = model_probe.run_probe(chunk_size=(128, 256))\n",
    "\n",
    "    plot_channels(\n",
    "        result_new_probe[\"grads_norm\"].cpu().numpy(),\n",
    "        [\n",
    "            \"block_norm DCT gradient (Y)\",\n",
    "            \"block_norm DCT gradient (U)\",\n",
    "            \"block_norm DCT gradient (V)\",\n",
    "        ],\n",
    "        0,\n",
    "        show=True,\n",
    "        save=f\"fastseg{variant}_grads_norm_test_new.png\",\n",
    "        tight_layout=False,\n",
    "    )\n",
    "\n",
    "    print(f\"{variant}::      orig results:\", result_orig)\n",
    "    print(f\"{variant}::     probe results:\", list(result_probe.keys()), result_probe[\"W\"])\n",
    "    print(f\"{variant}:: new  orig results:\", result_new_orig)\n",
    "    print(f\"{variant}:: new probe results:\", list(result_new_probe.keys()), result_new_probe[\"W\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write result to JSON\n",
    "import json\n",
    "\n",
    "res2 = res.copy()\n",
    "\n",
    "for k, v in res.items():\n",
    "    res2[k] = res[k].tolist()\n",
    "\n",
    "# with open(\"res_yuv_fixed.json\", \"w\") as wf:\n",
    "#     json.dump(res2, wf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from JSON\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"res_yuv.json\", \"r\") as rf:\n",
    "    res = json.load(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ious = res[\"ious\"][1:]\n",
    "ious_g = res[\"ious_g\"][1:]\n",
    "\n",
    "ious = np.array(ious) * 100\n",
    "ious_g = np.array(ious_g) * 100\n",
    "\n",
    "K = np.arange(1, 65)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(K, ious, label=\"baseline\")\n",
    "plt.plot(K, ious_g, label=\"gradients\")\n",
    "plt.plot(K, [res[\"iou_orig\"][0] * 100] * len(K), label=\"reference\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"mIoU (%)\")\n",
    "plt.legend()\n",
    "\n",
    "diff = np.array(ious_g) - np.array(ious)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(K, [0.0] * len(K), label=\"zero\")\n",
    "plt.plot(K, diff, label=\"diff\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"mIoU_grad - mIoU_base (pp)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_norms = fastseg.main(\n",
    "    mode=\"probe\",\n",
    "    device=device,\n",
    "    num_batches=None,\n",
    "    batch_size=batch_size_probe,\n",
    "    num_workers=0,\n",
    "    color_space=color_space,\n",
    "    do_print=True,\n",
    "    show_plots=show_plots,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating bitmap based on gradients\n",
    "import torch\n",
    "\n",
    "grad_norms_yuv = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                6.8752e-04,\n",
    "                1.0005e-03,\n",
    "                8.8672e-04,\n",
    "                8.1265e-04,\n",
    "                7.7627e-04,\n",
    "                5.3197e-04,\n",
    "                2.9156e-04,\n",
    "                1.4191e-04,\n",
    "            ],\n",
    "            [\n",
    "                7.6486e-04,\n",
    "                7.0540e-04,\n",
    "                7.1268e-04,\n",
    "                6.7172e-04,\n",
    "                5.6181e-04,\n",
    "                3.7691e-04,\n",
    "                2.3673e-04,\n",
    "                1.5010e-04,\n",
    "            ],\n",
    "            [\n",
    "                8.0659e-04,\n",
    "                7.2506e-04,\n",
    "                7.9096e-04,\n",
    "                6.9572e-04,\n",
    "                5.0154e-04,\n",
    "                3.1885e-04,\n",
    "                2.1427e-04,\n",
    "                1.6550e-04,\n",
    "            ],\n",
    "            [\n",
    "                7.9612e-04,\n",
    "                6.7133e-04,\n",
    "                6.5953e-04,\n",
    "                5.8854e-04,\n",
    "                4.3853e-04,\n",
    "                3.0538e-04,\n",
    "                2.1175e-04,\n",
    "                1.4944e-04,\n",
    "            ],\n",
    "            [\n",
    "                7.4625e-04,\n",
    "                5.3806e-04,\n",
    "                4.3351e-04,\n",
    "                4.0188e-04,\n",
    "                3.5539e-04,\n",
    "                2.7602e-04,\n",
    "                1.9974e-04,\n",
    "                1.4987e-04,\n",
    "            ],\n",
    "            [\n",
    "                5.0461e-04,\n",
    "                3.5121e-04,\n",
    "                2.7269e-04,\n",
    "                2.6673e-04,\n",
    "                2.4662e-04,\n",
    "                2.0697e-04,\n",
    "                1.6101e-04,\n",
    "                1.5607e-04,\n",
    "            ],\n",
    "            [\n",
    "                2.7449e-04,\n",
    "                2.0482e-04,\n",
    "                1.9560e-04,\n",
    "                1.9627e-04,\n",
    "                1.7156e-04,\n",
    "                1.3907e-04,\n",
    "                1.2345e-04,\n",
    "                1.6210e-04,\n",
    "            ],\n",
    "            [\n",
    "                1.3655e-04,\n",
    "                1.2796e-04,\n",
    "                1.3620e-04,\n",
    "                1.4445e-04,\n",
    "                1.5163e-04,\n",
    "                1.3441e-04,\n",
    "                1.0513e-04,\n",
    "                1.0732e-04,\n",
    "            ],\n",
    "        ],\n",
    "        [\n",
    "            [\n",
    "                3.6655e-04,\n",
    "                2.3057e-04,\n",
    "                1.7865e-04,\n",
    "                1.8027e-04,\n",
    "                1.6247e-04,\n",
    "                1.1040e-04,\n",
    "                6.7999e-05,\n",
    "                7.4597e-05,\n",
    "            ],\n",
    "            [\n",
    "                3.8346e-04,\n",
    "                2.2649e-04,\n",
    "                1.6651e-04,\n",
    "                1.3307e-04,\n",
    "                1.0197e-04,\n",
    "                6.6842e-05,\n",
    "                5.2780e-05,\n",
    "                8.2484e-05,\n",
    "            ],\n",
    "            [\n",
    "                2.7195e-04,\n",
    "                1.7968e-04,\n",
    "                1.3611e-04,\n",
    "                9.7587e-05,\n",
    "                6.4875e-05,\n",
    "                4.3859e-05,\n",
    "                4.0964e-05,\n",
    "                6.0591e-05,\n",
    "            ],\n",
    "            [\n",
    "                1.6147e-04,\n",
    "                1.1255e-04,\n",
    "                8.4427e-05,\n",
    "                5.8382e-05,\n",
    "                3.6385e-05,\n",
    "                2.4421e-05,\n",
    "                2.7155e-05,\n",
    "                3.5520e-05,\n",
    "            ],\n",
    "            [\n",
    "                1.1307e-04,\n",
    "                8.1603e-05,\n",
    "                5.9668e-05,\n",
    "                3.9579e-05,\n",
    "                2.4929e-05,\n",
    "                1.7698e-05,\n",
    "                2.1803e-05,\n",
    "                2.5623e-05,\n",
    "            ],\n",
    "            [\n",
    "                1.0168e-04,\n",
    "                6.4970e-05,\n",
    "                4.5855e-05,\n",
    "                3.1443e-05,\n",
    "                1.9750e-05,\n",
    "                1.8425e-05,\n",
    "                2.3671e-05,\n",
    "                2.8941e-05,\n",
    "            ],\n",
    "            [\n",
    "                9.7706e-05,\n",
    "                5.5146e-05,\n",
    "                3.9245e-05,\n",
    "                3.0614e-05,\n",
    "                2.4767e-05,\n",
    "                2.0179e-05,\n",
    "                2.4544e-05,\n",
    "                3.2968e-05,\n",
    "            ],\n",
    "            [\n",
    "                9.7746e-05,\n",
    "                6.4203e-05,\n",
    "                4.7085e-05,\n",
    "                4.9695e-05,\n",
    "                4.7705e-05,\n",
    "                3.3300e-05,\n",
    "                3.0521e-05,\n",
    "                2.8349e-05,\n",
    "            ],\n",
    "        ],\n",
    "        [\n",
    "            [\n",
    "                3.3019e-04,\n",
    "                2.4871e-04,\n",
    "                1.8138e-04,\n",
    "                1.3581e-04,\n",
    "                9.3264e-05,\n",
    "                7.3249e-05,\n",
    "                8.1976e-05,\n",
    "                1.0235e-04,\n",
    "            ],\n",
    "            [\n",
    "                3.8119e-04,\n",
    "                2.0982e-04,\n",
    "                1.2904e-04,\n",
    "                8.5882e-05,\n",
    "                5.9820e-05,\n",
    "                4.8262e-05,\n",
    "                6.1512e-05,\n",
    "                1.1707e-04,\n",
    "            ],\n",
    "            [\n",
    "                3.0196e-04,\n",
    "                1.7015e-04,\n",
    "                1.1067e-04,\n",
    "                6.9676e-05,\n",
    "                4.1495e-05,\n",
    "                3.7797e-05,\n",
    "                4.9818e-05,\n",
    "                9.3241e-05,\n",
    "            ],\n",
    "            [\n",
    "                1.9895e-04,\n",
    "                1.3512e-04,\n",
    "                1.0428e-04,\n",
    "                6.9022e-05,\n",
    "                3.6996e-05,\n",
    "                3.4375e-05,\n",
    "                3.8280e-05,\n",
    "                5.0030e-05,\n",
    "            ],\n",
    "            [\n",
    "                1.6139e-04,\n",
    "                1.1244e-04,\n",
    "                8.7535e-05,\n",
    "                6.2750e-05,\n",
    "                4.0694e-05,\n",
    "                3.2123e-05,\n",
    "                3.2318e-05,\n",
    "                3.3000e-05,\n",
    "            ],\n",
    "            [\n",
    "                1.2854e-04,\n",
    "                9.0579e-05,\n",
    "                6.8180e-05,\n",
    "                5.0702e-05,\n",
    "                3.7586e-05,\n",
    "                2.6411e-05,\n",
    "                3.1091e-05,\n",
    "                3.9480e-05,\n",
    "            ],\n",
    "            [\n",
    "                1.3134e-04,\n",
    "                8.2332e-05,\n",
    "                5.7584e-05,\n",
    "                3.9073e-05,\n",
    "                2.8568e-05,\n",
    "                2.1731e-05,\n",
    "                3.1127e-05,\n",
    "                5.1141e-05,\n",
    "            ],\n",
    "            [\n",
    "                1.1424e-04,\n",
    "                8.4455e-05,\n",
    "                6.8010e-05,\n",
    "                4.6614e-05,\n",
    "                2.6990e-05,\n",
    "                2.1787e-05,\n",
    "                3.3424e-05,\n",
    "                4.4609e-05,\n",
    "            ],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "nsend = 8\n",
    "print(\"K: \", nsend, \", CR: \", nsend / 64)\n",
    "\n",
    "grad_norms_y = grad_norms_yuv[0].reshape(-1)\n",
    "\n",
    "_, top_chunks_indices = grad_norms_y.topk(nsend)\n",
    "print(top_chunks_indices)\n",
    "top_chunks_indices, _ = top_chunks_indices.sort()\n",
    "print(top_chunks_indices)\n",
    "\n",
    "bitmap = torch.zeros_like(grad_norms_y, dtype=torch.int)\n",
    "bitmap[top_chunks_indices] = 1\n",
    "bitmap = bitmap.reshape(8, 8)\n",
    "\n",
    "print(bitmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from models.model import plot_channels\n",
    "plot_channels(\n",
    "    grad_norms.cpu().numpy(), \n",
    "    [\n",
    "        \"norm DCT gradient (Y)\",\n",
    "        \"norm DCT gradient (U)\",\n",
    "        \"norm DCT gradient (V)\", \n",
    "    ], \n",
    "    0, \n",
    "    show=True,\n",
    "    save=\"test.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient-optimized chunk selection \n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2 \n",
    "%reset -f\n",
    "%matplotlib inline\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "# Force all new tensors to be created on cuda device\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from models import run_model\n",
    "from models.model import plot_channels\n",
    "\n",
    "\n",
    "config_yolov8 = {\n",
    "    \"name\": \"yolov8\",\n",
    "    \"variant\": \"n\",\n",
    "    \"task\": \"detect\",\n",
    "    \"snapshot\": \"yolov8n.pt\",\n",
    "    \"unit\": \"mAP_50_95\",\n",
    "}\n",
    "\n",
    "config_fastseg = {\n",
    "    \"name\": \"fastseg\",\n",
    "    \"variant\": \"small\",\n",
    "    \"snapshot\": Path.home() / \"data/models/fastseg/raw/small/best_checkpoint_ep171.pth\",\n",
    "    # \"variant\": \"large\",\n",
    "    # \"snapshot\": Path.home() / \"data/models/fastseg/raw/large/best_checkpoint_ep172.pth\"\n",
    "    \"unit\": \"mean_iu\",\n",
    "}\n",
    "\n",
    "config = config_fastseg\n",
    "\n",
    "color_space = \"yuv\"\n",
    "K = [2, 4, 8, 16, 32, 64, 128, 192] #range(8, 9)\n",
    "lvc_params = [\n",
    "    {\n",
    "        \"packet_loss\": None,\n",
    "        \"seed\": 42,\n",
    "        \"mode\": 444,\n",
    "        \"cr\": k / (64 * 3),\n",
    "        \"csnr_db\": \"inf\",\n",
    "        \"estimator\": \"zf\",\n",
    "        \"nchunks\": 64,\n",
    "        \"color_space\": color_space,\n",
    "    }\n",
    "    for k in K\n",
    "]\n",
    "\n",
    "# device = set_device(\"cuda:0\")\n",
    "num_batches = None\n",
    "batch_size = 16\n",
    "num_batches_probe = 8\n",
    "batch_size_probe = 8\n",
    "\n",
    "do_print = False\n",
    "show_plots = False\n",
    "\n",
    "file_name = f'{config[\"name\"]}{config[\"variant\"]}_{color_space}_probe{num_batches_probe}nb{batch_size_probe}bs'\n",
    "\n",
    "# Setup for parallel processing\n",
    "ngpus = 1\n",
    "ngpus = min(len(lvc_params), ngpus)\n",
    "ncpus = cpu_count()\n",
    "\n",
    "cpus = np.array_split(range(ncpus), ngpus)\n",
    "K_groups = np.array_split(K, ngpus)\n",
    "lvc_params_groups = np.array_split(lvc_params, ngpus)\n",
    "\n",
    "configs = [config] * ngpus\n",
    "devices = [f\"cuda:{i}\" for i in range(ngpus)]\n",
    "color_spaces = [color_space] * ngpus\n",
    "ranks = range(ngpus)\n",
    "num_workers_groups = [0] * ngpus\n",
    "num_batches_groups = [num_batches] * ngpus\n",
    "batch_size_groups = [batch_size] * ngpus\n",
    "num_batches_probe_groups = [num_batches_probe] * ngpus\n",
    "batch_size_probe_groups = [batch_size_probe] * ngpus\n",
    "do_print_groups = [do_print] * ngpus\n",
    "show_plots_groups = [show_plots] * ngpus\n",
    "\n",
    "results_groups = []\n",
    "\n",
    "with Pool(processes=ngpus) as pool:\n",
    "    results_groups = pool.starmap(\n",
    "        run_model,\n",
    "        zip(\n",
    "            configs,\n",
    "            devices,\n",
    "            lvc_params_groups,\n",
    "            color_spaces,\n",
    "            ranks,\n",
    "            cpus,\n",
    "            num_workers_groups,\n",
    "            num_batches_groups,\n",
    "            batch_size_groups,\n",
    "            num_batches_probe_groups,\n",
    "            batch_size_probe_groups,\n",
    "            do_print_groups,\n",
    "            show_plots_groups, \n",
    "        ),\n",
    "    )\n",
    "\n",
    "torch.save(results_groups, f\"{file_name}.pt\")\n",
    "plot_channels(results_groups[0][\"grads_norm\"], \n",
    "    [\n",
    "        f\"norm DCT gradient ({color_space[0].upper()})\",\n",
    "        f\"norm DCT gradient ({color_space[1].upper()})\",\n",
    "        f\"norm DCT gradient ({color_space[2].upper()})\",\n",
    "    ],\n",
    "    0,\n",
    "    show=True,\n",
    "    save=f\"{file_name}_grads_norm.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res = {}\n",
    "score_orig = []\n",
    "\n",
    "for results_group in results_groups:\n",
    "    score_orig.append(results_group[\"orig\"][config[\"unit\"]])\n",
    "\n",
    "    for params, results_lvc, results_lvc_g in zip(\n",
    "        results_group[\"lvc_params\"],\n",
    "        results_group[\"lvc\"],\n",
    "        results_group[\"lvc_g\"],\n",
    "    ):\n",
    "        k = int(params[\"cr\"] * params[\"nchunks\"])\n",
    "        unit = config[\"unit\"]\n",
    "        score = results_lvc[unit]\n",
    "        score_g = results_lvc_g[unit]\n",
    "\n",
    "        res[k] = {}\n",
    "        res[k][\"score\"] = score\n",
    "        res[k][\"score_g\"] = score_g\n",
    "\n",
    "res = {k: res[k] for k in sorted(res.keys())}\n",
    "scores = [val[\"score\"] * 100 for val in res.values()]\n",
    "scores_g = [val[\"score_g\"] * 100 for val in res.values()]\n",
    "\n",
    "print(score_orig)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(res.keys(), scores, label=\"base\")\n",
    "plt.plot(res.keys(), scores_g, label=\"grad\")\n",
    "plt.plot(res.keys(), [score_orig[0] * 100] * len(res), label=\"reference\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(config[\"unit\"])\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(res.keys(), [0.0] * len(res), label=\"zero\")\n",
    "plt.plot(res.keys(), np.array(scores_g) - np.array(scores), label=\"grad - base\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(f\"{config['unit']}_grad - {config['unit']}_base\")\n",
    "\n",
    "plt.savefig(f\"{file_name}_score.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_orig[0] * 100, scores[-1], scores_g[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# torch.save(results_groups, \"res2.pt\")\n",
    "torch.save(results_groups, f\"{file_name}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "771a0f959a60da4e11c1bde91c92923dee3f813a78caf816e9ab6aaf7cb3299e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
