{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sionna as sn\n",
    "\n",
    "%matplotlib inline\n",
    "# also try %matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BITS_PER_SYMBOL = 2  # QPSK\n",
    "constellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL)\n",
    "\n",
    "constellation.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = sn.mapping.Mapper(constellation=constellation)\n",
    "\n",
    "# The demapper uses the same constellation object as the mapper\n",
    "demapper = sn.mapping.Demapper(\"app\", constellation=constellation)\n",
    "\n",
    "binary_source = sn.utils.BinarySource()\n",
    "awgn_channel = sn.channel.AWGN()\n",
    "\n",
    "no = sn.utils.ebnodb2no(\n",
    "    ebno_db=10.0, num_bits_per_symbol=NUM_BITS_PER_SYMBOL, coderate=1.0\n",
    ")  # Coderate set to 1 as we do uncoded transmission here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # How many examples are processed by Sionna in parallel\n",
    "\n",
    "bits = binary_source([BATCH_SIZE, 1024])  # Blocklength\n",
    "print(\"Shape of bits: \", bits.shape)\n",
    "\n",
    "x = mapper(bits)\n",
    "print(\"Shape of x: \", x.shape)\n",
    "\n",
    "y = awgn_channel([x, no])\n",
    "print(\"Shape of y: \", y.shape)\n",
    "\n",
    "llr = demapper([y, no])\n",
    "print(\"Shape of llr: \", llr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 8  # how many samples shall be printed\n",
    "num_symbols = int(num_samples / NUM_BITS_PER_SYMBOL)\n",
    "\n",
    "print(f\"First {num_samples} transmitted bits: {bits[0,:num_samples]}\")\n",
    "print(f\"First {num_symbols} transmitted symbols: {np.round(x[0,:num_symbols], 2)}\")\n",
    "print(f\"First {num_symbols} received symbols: {np.round(y[0,:num_symbols], 2)}\")\n",
    "print(f\"First {num_samples} demapped llrs: {np.round(llr[0,:num_samples], 2)}\")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axes().set_aspect(1)\n",
    "plt.grid(True)\n",
    "plt.title(\"Channel output\")\n",
    "plt.xlabel(\"Real Part\")\n",
    "plt.ylabel(\"Imaginary Part\")\n",
    "plt.scatter(tf.math.real(y), tf.math.imag(y))\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncodedSystemAWGN(Model):  # Inherits from Keras Model\n",
    "    def __init__(self, num_bits_per_symbol, block_length):\n",
    "        \"\"\"\n",
    "        A keras model of an uncoded transmission over the AWGN channel.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_bits_per_symbol: int\n",
    "            The number of bits per constellation symbol, e.g., 4 for QAM16.\n",
    "\n",
    "        block_length: int\n",
    "            The number of bits per transmitted message block (will be the codeword length later).\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        batch_size: int\n",
    "            The batch_size of the Monte-Carlo simulation.\n",
    "\n",
    "        ebno_db: float\n",
    "            The `Eb/No` value (=rate-adjusted SNR) in dB.\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        (bits, llr):\n",
    "            Tuple:\n",
    "\n",
    "        bits: tf.float32\n",
    "            A tensor of shape `[batch_size, block_length] of 0s and 1s\n",
    "            containing the transmitted information bits.\n",
    "\n",
    "        llr: tf.float32\n",
    "            A tensor of shape `[batch_size, block_length] containing the\n",
    "            received log-likelihood-ratio (LLR) values.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()  # Must call the Keras model initializer\n",
    "\n",
    "        self.num_bits_per_symbol = num_bits_per_symbol\n",
    "        self.block_length = block_length\n",
    "        self.constellation = sn.mapping.Constellation(\"qam\", self.num_bits_per_symbol)\n",
    "        self.mapper = sn.mapping.Mapper(constellation=self.constellation)\n",
    "        self.demapper = sn.mapping.Demapper(\"app\", constellation=self.constellation)\n",
    "        self.binary_source = sn.utils.BinarySource()\n",
    "        self.awgn_channel = sn.channel.AWGN()\n",
    "\n",
    "    # @tf.function # Enable graph execution to speed things up\n",
    "    def __call__(self, batch_size, ebno_db):\n",
    "\n",
    "        # no channel coding used; we set coderate=1.0\n",
    "        no = sn.utils.ebnodb2no(\n",
    "            ebno_db, num_bits_per_symbol=self.num_bits_per_symbol, coderate=1.0\n",
    "        )\n",
    "\n",
    "        bits = self.binary_source(\n",
    "            [batch_size, self.block_length]\n",
    "        )  # Blocklength set to 1024 bits\n",
    "        x = self.mapper(bits)\n",
    "        y = self.awgn_channel([x, no])\n",
    "        llr = self.demapper([y, no])\n",
    "        return bits, llr\n",
    "\n",
    "\n",
    "model_uncoded_awgn = UncodedSystemAWGN(\n",
    "    num_bits_per_symbol=NUM_BITS_PER_SYMBOL, block_length=1024\n",
    ")\n",
    "\n",
    "EBN0_DB_MIN = -3.0  # Minimum value of Eb/N0 [dB] for simulations\n",
    "EBN0_DB_MAX = 5.0  # Maximum value of Eb/N0 [dB] for simulations\n",
    "BATCH_SIZE = 2000  # How many examples are processed by Sionna in parallel\n",
    "\n",
    "ber_plots = sn.utils.PlotBER(\"AWGN\")\n",
    "ber_plots.simulate(\n",
    "    model_uncoded_awgn,\n",
    "    ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 20),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_target_block_errors=100,  # simulate until 100 block errors occured\n",
    "    legend=\"Uncoded\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=100,  # run 100 Monte-Carlo simulations (each with batch_size samples)\n",
    "    show_fig=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEC\n",
    "\n",
    "k = 12\n",
    "n = 20\n",
    "\n",
    "encoder = sn.fec.ldpc.LDPC5GEncoder(k, n)\n",
    "decoder = sn.fec.ldpc.LDPC5GDecoder(encoder, hard_out=True)\n",
    "\n",
    "BATCH_SIZE = 1  # one codeword in parallel\n",
    "u = binary_source([BATCH_SIZE, k])\n",
    "print(\"Input bits are: \\n\", u.numpy())\n",
    "\n",
    "c = encoder(u)\n",
    "print(\"Encoded bits are: \\n\", c.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10  # samples per scenario\n",
    "num_basestations = 4\n",
    "num_users = 5  # users per basestation\n",
    "n = 1000  # codeword length per transmitted codeword\n",
    "coderate = 0.5  # coderate\n",
    "\n",
    "k = int(coderate * n)  # number of info bits per codeword\n",
    "\n",
    "# instantiate a new encoder for codewords of length n\n",
    "encoder = sn.fec.ldpc.LDPC5GEncoder(k, n)\n",
    "\n",
    "# the decoder must be linked to the encoder (to know the exact code parameters used for encoding)\n",
    "decoder = sn.fec.ldpc.LDPC5GDecoder(\n",
    "    encoder,\n",
    "    hard_out=True,  # binary output or provide soft-estimates\n",
    "    return_infobits=True,  # or also return (decoded) parity bits\n",
    "    num_iter=20,  # number of decoding iterations\n",
    "    cn_type=\"boxplus-phi\",\n",
    ")  # also try \"minsum\" decoding\n",
    "\n",
    "# draw random bits to encode\n",
    "u = binary_source([BATCH_SIZE, num_basestations, num_users, k])\n",
    "print(\"Shape of u: \", u.shape)\n",
    "\n",
    "# We can immediately encode u for all users, basetation and samples\n",
    "# This all happens with a single line of code\n",
    "c = encoder(u)\n",
    "print(\"Shape of c: \", c.shape)\n",
    "\n",
    "print(\"Total number of processed bits: \", np.prod(c.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64\n",
    "n = 128\n",
    "\n",
    "encoder = sn.fec.polar.Polar5GEncoder(k, n)\n",
    "decoder = sn.fec.polar.Polar5GDecoder(encoder, dec_type=\"SCL\")  # you can also use \"SCL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodedSystemAWGN(Model):  # Inherits from Keras Model\n",
    "    def __init__(self, num_bits_per_symbol, n, coderate):\n",
    "        super().__init__()  # Must call the Keras model initializer\n",
    "\n",
    "        self.num_bits_per_symbol = num_bits_per_symbol\n",
    "        self.n = n\n",
    "        self.k = int(n * coderate)\n",
    "        self.coderate = coderate\n",
    "        self.constellation = sn.mapping.Constellation(\"qam\", self.num_bits_per_symbol)\n",
    "\n",
    "        self.mapper = sn.mapping.Mapper(constellation=self.constellation)\n",
    "        self.demapper = sn.mapping.Demapper(\"app\", constellation=self.constellation)\n",
    "\n",
    "        self.binary_source = sn.utils.BinarySource()\n",
    "        self.awgn_channel = sn.channel.AWGN()\n",
    "\n",
    "        self.encoder = sn.fec.ldpc.LDPC5GEncoder(self.k, self.n)\n",
    "        self.decoder = sn.fec.ldpc.LDPC5GDecoder(self.encoder, hard_out=True)\n",
    "\n",
    "    @tf.function # activate graph execution to speed things up\n",
    "    def __call__(self, batch_size, ebno_db):\n",
    "        no = sn.utils.ebnodb2no(\n",
    "            ebno_db,\n",
    "            num_bits_per_symbol=self.num_bits_per_symbol,\n",
    "            coderate=self.coderate,\n",
    "        )\n",
    "\n",
    "        bits = self.binary_source([batch_size, self.k])\n",
    "        codewords = self.encoder(bits)\n",
    "        x = self.mapper(codewords)\n",
    "        y = self.awgn_channel([x, no])\n",
    "        llr = self.demapper([y, no])\n",
    "        bits_hat = self.decoder(llr)\n",
    "        return bits, bits_hat\n",
    "\n",
    "\n",
    "CODERATE = 0.5\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "model_coded_awgn = CodedSystemAWGN(\n",
    "    num_bits_per_symbol=NUM_BITS_PER_SYMBOL, n=2048, coderate=CODERATE\n",
    ")\n",
    "\n",
    "bits, bits_hat = model_coded_awgn(BATCH_SIZE, 5)\n",
    "print(\"bits    : \", bits[0][0])\n",
    "print(\"bits_hat: \", bits_hat[0][0])\n",
    "\n",
    "ber_plots.simulate(\n",
    "    model_coded_awgn,\n",
    "    ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 15),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_target_block_errors=100,\n",
    "    legend=\"Coded\",\n",
    "    soft_estimates=False,\n",
    "    max_mc_iter=10,\n",
    "    show_fig=True,\n",
    "    forward_keyboard_interrupt=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()  # enables graph-mode of the following function\n",
    "def run_graph(batch_size, ebno_db):\n",
    "    # all code inside this function will be executed in graph mode, also calls of other functions\n",
    "    print(\n",
    "        f\"Tracing run_graph for values batch_size={batch_size} and ebno_db={ebno_db}.\"\n",
    "    )  # print whenever this function is traced\n",
    "    return model_coded_awgn(batch_size, ebno_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10  # try also different batch sizes\n",
    "ebno_db = 1.5\n",
    "\n",
    "# run twice - how does the output change?\n",
    "run_graph(batch_size, ebno_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can print the cached signatures with\n",
    "print(run_graph.pretty_printed_concrete_signatures())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = 4  # average over multiple runs\n",
    "batch_size = BATCH_SIZE  # try also different batch sizes\n",
    "ebno_db = 1.5\n",
    "\n",
    "# --- eager mode ---\n",
    "t_start = time.perf_counter()\n",
    "for _ in range(repetitions):\n",
    "    bits, bits_hat = model_coded_awgn(\n",
    "        tf.constant(batch_size, tf.int32), tf.constant(ebno_db, tf.float32)\n",
    "    )\n",
    "t_stop = time.perf_counter()\n",
    "# throughput in bit/s\n",
    "throughput_eager = np.size(bits.numpy()) * repetitions / (t_stop - t_start) / 1e6\n",
    "\n",
    "print(f\"Throughput in Eager mode: {throughput_eager :.3f} Mbit/s\")\n",
    "# --- graph mode ---\n",
    "# run once to trace graph (ignored for throughput)\n",
    "run_graph(tf.constant(batch_size, tf.int32), tf.constant(ebno_db, tf.float32))\n",
    "\n",
    "t_start = time.perf_counter()\n",
    "for _ in range(repetitions):\n",
    "    bits, bits_hat = run_graph(\n",
    "        tf.constant(batch_size, tf.int32), tf.constant(ebno_db, tf.float32)\n",
    "    )\n",
    "t_stop = time.perf_counter()\n",
    "# throughput in bit/s\n",
    "throughput_graph = np.size(bits.numpy()) * repetitions / (t_stop - t_start) / 1e6\n",
    "\n",
    "print(f\"Throughput in graph mode: {throughput_graph :.3f} Mbit/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ber_plots.simulate(\n",
    "    run_graph,\n",
    "    ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 12),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_target_block_errors=500,\n",
    "    legend=\"Coded (Graph mode)\",\n",
    "    soft_estimates=True,\n",
    "    max_mc_iter=100,\n",
    "    show_fig=True,\n",
    "    forward_keyboard_interrupt=False,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digcom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c472a14bf945cb6b8df3eaf24e9ff4437931373f0a090bc77905e366329f5ffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
